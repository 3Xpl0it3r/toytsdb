#### TSDB 设计的目的和解决的问题
&emsp;tsdb 是一堆(时间:数据)的集合, 随着时间的退役数据增加,主要用来观察数据的变化: 主要有以下几个特征:
- 海量的数据
- 只读(append only)
- 按照时间排序
- Accessed bulk
- Most recent first
- 基数大


*海量的数据*
&emsp; 根据时许数据库的性质，单个时间点数据几乎是没有什么意义的，当只有大量的数据才会有意义. 数据周期采集，在某些行业这个数据可能非常大.
- 采集的速率直接影响写的性能(尽管文件描述符的上限是瓶颈)
- 写放大问题，我们需要优化写过程，以便减少磁盘的占用。

*Append Only*
- 每个数据点是不可以修改的。(删除只能针对最近的数据，而不能针对某一个特色的时间点数据)

*按照时间排序的*
&emsp; 时间点默认是按照时间戳排序的，因此可以几乎不需要任何的代价来构建索引


*Accessed Bulk*
&emsp;一般情况下我们读取的是一段时间段里面的数据，当我们读取数据的时候可以通过来改善局部性（prometheus 针对读问题，每个series都单独一个文件, v1版本)

*Most recent first*
&emsp;其实大部分都只关注最近的数据怎么样，因此我们可以针对cache做一些事情.

*High cardinality*
&emsp;时间序列往往有较高的基础,(包括series tories)。 针对每个series单独创建一个文件由太浪费了。


#### 解决方案
&emsp;一般情况下我们发现时序数据库写入很频繁，在一个很小的时间段里面会写的很频繁
&emsp;Google的leveldb是作为kv存储引擎来用的。并且也有大量的go实现，leveldb就是基于LSM树来实现的。LSM只能顺序的写入尾部，它对于这种追加的方式工作的非常好,他的按键排序的方式也非常的适合这种时序数据库。事实上早起的prometheus 和 tsdb  也是基于leveldb来做的。
&emsp;然而leveldb在某些地方会造成巨大的浪费,正如我们后续看到的时序数据是单调递增的，针对这个特性我们可以做一些压缩。另外针对value其实大部分变动都很小，这种这个也可以做一些压缩.


#### 什么database 引擎
l


#### 数据模型
&emsp;根据上面特性TSDB采用了线性的数据存储模型结构，将数据按照时间来分割。每个分区都是扮演一个完全独立的数据库(针对那个时间段)。
```text
  │                 │
Read              Write
  │                 │
  │                 V
  │      ┌───────────────────┐ max: 1600010800
  ├─────>   Memory Partition
  │      └───────────────────┘ min: 1600007201
  │
  │      ┌───────────────────┐ max: 1600007200
  ├─────>   Memory Partition
  │      └───────────────────┘ min: 1600003601
  │
  │      ┌───────────────────┐ max: 1600003600
  └─────>   Disk Partition
         └───────────────────┘ min: 1600000000
```

&emsp;只有Head和下一个分区才是可以写的(head类型wal)。这个被成为memory  partiation, 数据在被持久化之前为了放置丢失，他们会被优先写入到(write Ahead log)

&emsp;老得分区会被写入到磁盘上一个单独的文件里面。这个被成为磁盘分区，磁盘分支只能读，不能写。文件是通过mmap机制来写入到磁盘文件的。

&emsp;我们可以设置时间周期，到了一定周期后 memory partition会被追加到ahead里面，老得ahead会被持久化到文件。

&emsp;另外时序数据库的特点是一个分区写入一个文件。
- 任何写文件几乎都没有写放大的问题,因为他们是顺序的追加进去的。
- 文件的数量不依赖于metric的基数
- 提升了读取的局部型，因为他们通常只读取某一个时间段内的数据




#### 内存分区
*data point list*
&emsp;list of data point在内存上就像heap上的一个列表(从技术层面角度来说这个就像go里面的slice).这个看起来是一个没有限制的数据点，咋一看似乎链表看起来最合理，他可以以O(1)复杂度来写数据，但是在内存里面相邻排列的数据可以得益于内存局部性缓存的优势。另外其实TSDB的数据是实现排好序的，因此传统的搜索依然很有效。

*Out of Data Point*
&emsp;在真实的应用里面，由于网络时钟导致了数据错乱这个其实也是比较常见的。当写数据这个因素也应该被考虑进去，，因为数据需要保持被排好序的(在prometheus tsdb里面会有对数据时间点做校验)

&emsp;主要注意的是，当我们写数据的时候我们需要用排他锁，但是锁是很消耗性能的，因此我们需要一个比较好的方式来避免锁的问题。

&emsp;无须的数据可以被拆分为两种:
- 这个无序的数据所对应的时间仍然在我们分区里面
>  *这种情况下，我们可以先把数据存储到一个独立的无序的数组里面，当然数据开始要持久化的时候，我们将两个list merge成一个list, resort，然后再持久化到文件里面*
- 这个无序的数据所对应的时候不在我们分区里面。

&emsp;在data model那段，提到了只有head和next partition才是在heap上的，针对第二种情况下，一旦新的分区被添加到ahead里面，跨分区的数据点就可以被写入进去，为了解决第二种情况，我们需要保持两个最新的分区都是可写的，我们避免这额被丢弃。


#### Write Ahead Log
&emsp;由于内存的是易失性存储，有可能突然断电/系统crash等导致内存里面数据丢失了。为了解决这种情况，我们需要先吧数据写入到一个Write Ahead Log里面。即使crash掉了，我们仍然可以将数据恢复出来。
&emsp;为了支持数据库提供更新操作，WAL必须支持非常底层的操作，来完全恢复这个更新的操作。需要准确的知道磁盘快里面那个字节被修改了。

&emsp;然而time seria数据是只追加，tsstorage只允许所有的磁盘分区只读，它需要存储一些写入memory内存的里面数据点的一些高层的概况，以便它可以通过一种建议的磁盘格式来恢复他们。


#### 磁盘分区
&emsp;最快的理解磁盘分区的方式就是查看他们在文件系统上一个宏观的分布情况。

&emsp;正如我们下面所看到的，针对每个分区使用一个单独的目录，里面有一个meatadata文件，下面的都是一些底层数据，下面是i一个非常简易的prometheus 存储布局
```text
$ tree ./data
./data
├── p-1600000001-1600003600
│   ├── data
│   └── meta.json
├── p-1600003601-1600007200
│   ├── data
│   └── meta.json
└── p-1600007201-1600010800
    ├── data
    └── meta.json

```
#### memory mapped data
&emsp;正如上面所看到的那样，所有的数据点都存放在一个单独的文件里面。tstorage存储布局如下:
```text
    ┌───────────────────────────┐
    │    Metric-1    │ Metric-2 │
    │───────────────────────────│
    │ Metric-3 │                │
    │──────────┘                │
    │          Metric-4         │
    │───────────────────────────│
    │   Metric-5  │   Metric-6  │
    │───────────────────────────│
    │         Metric-7      │   │
    │───────────────────────┘   │
    │         Metric-8          │
    │───────────────────────────│
    │Metric-9│     Metric-10    │
    └───────────────────────────┘
             File format
```
&emsp;Metrics-0~Metrics10 分别代表对应metrics的数据点。
&emsp;我们在回忆下数据库的特征，DataPoint是不可修改的，大部分情况下度量的值都是通过一个指定的范围内获取得到，因此我们可以对数据点通过metrics来归档来提高局部
&emsp;这个文件以memory map的方式来透明的缓存起来，这个可以让我们缓存文件，而不需要先把数据copy到用户态。这个非常有效的。，这个问题最初解决随着时间推移，它会慢慢消耗堆内存，
&emsp;Memory mappped file可以被认为是go的[]byte。
```go
type diskPartition struct {
	// file descriptor of data file
	f *os.File
	// memory-mapped file backed by f
	mappedFile []byte
```
&emsp;但是我们如何检索这样的数据呢，这个可以通过直接吧数据copy到heap上，然后unmarshal到结构体，但是这个就违背了内存映射的初衷了，事实上我们需要一个有效的索引结构来访问被编码后的数据。下面介绍的远数据就用到了。


#### Metadata 
&emsp;metadata 的内容看起来如下:, 一个partition只有一个metadata，我们采用了json的格式，因为这个非常易于编程语言来处理。
```json
{
  "minTimestamp": 1600000001,
  "maxTimestamp": 1600003600,
  "numDataPoints": 7200,
  "metrics": {
    "metric-1": {
      "name": "metric-1",
      "offset": 0,
      "minTimestamp": 1600000001,
      "maxTimestamp": 1600003600,
      "numDataPoints": 3600
    },
    "metric-2": {
      "name": "metric-2",
      "offset": 36014,
      "minTimestamp": 1600000001,
      "maxTimestamp": 1600003600,
      "numDataPoints": 3600
    }
  }
}
```
&emsp;metadata 是用来构建partition索引的。这是一个存储每个metrics的字节大小和偏移辆地址的地方。有了这些tstroage就可以对内存里面的数据进行随机访问了。
```text
     {
       "minTimestamp": 1600000001,
       "maxTimestamp": 1600003600,
       "numDataPoints": 7200,
       "metrics": {
         "metric-1": {
           "name": "metric-1",
   ┌─────  "offset": 0,
   │       "minTimestamp": 1600000001,
   │       "maxTimestamp": 1600003600,
   │       "numDataPoints": 3600
   │     },
   │     "metric-2": {
   │       "name": "metric-2",
   │       "offset": 36014, ────────────┐
   │       "minTimestamp": 1600000001,  │
   │       "maxTimestamp": 1600003600,  │
   │       "numDataPoints": 3600        │
   │     }                              │
   │   }                                │
   │ }              ┌───────────────────┘
   │                │
   V                V
   0              36014
   ┌───────────────────────────┐
   │    Metric-1    │ Metric-2 │
   │───────────────────────────│
   │ Metric-3 │                │
   │──────────┘                │
   │          Metric-4         │
   │───────────────────────────│
   │   Metric-5  │   Metric-6  │
   │───────────────────────────│
   │         Metric-7      │   │
   │───────────────────────┘   │
   │         Metric-8          │
   │───────────────────────────│
   │Metric-9│     Metric-10    │
   └───────────────────────────┘
```
&emsp;为了记录每个metrics的起始偏移量，当我们需要flush到磁盘的时候，我们需要记录下它。它对每个metrics的数据点进行编码转换，并且在构建索引的时候写入
